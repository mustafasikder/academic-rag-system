{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End RAG Demo: Academic Paper Q&A\n",
    "\n",
    "This notebook demonstrates a complete RAG (Retrieval-Augmented Generation) pipeline for answering questions about research papers.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **PDF Extraction** → Extract text from PDF\n",
    "2. **Text Cleaning** → Remove artifacts and boilerplate\n",
    "3. **Adaptive Chunking** → Split into semantic chunks\n",
    "4. **Hybrid Retrieval** → Dense (embeddings) + Sparse (BM25)\n",
    "5. **Answer Generation** → LLM synthesizes answer from retrieved context\n",
    "\n",
    "**Expected Runtime:** 5-10 minutes (first run downloads models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pymupdf sentence-transformers faiss-cpu transformers accelerate nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "print(\"Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')  # Adjust if running from different directory\n",
    "\n",
    "from academic_rag_system.preprocessing.pdf_cleaner import clean_academic_text\n",
    "from academic_rag_system.preprocessing.adaptive_chunker import AcademicPaperChunker\n",
    "from academic_rag_system.retrieval.hybrid_retriever import HybridRetriever\n",
    "from academic_rag_system.generation.answer_generator import RAGAnswerGenerator\n",
    "\n",
    "print(\"Modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: C:\\Users\\Abu.Sikder\\OneDrive - FDA\\data_move_june_2025\\FDA\\EduRAG\\dumps\\s13031-018-0151-3.pdf\n",
      "Extracted 53402 characters from 13 pages\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Path to your PDF\n",
    "# pdf_path = r\"path/to/your/paper.pdf\"  # ← Change this to your PDF path\n",
    "pdf_path = r\"C:\\Users\\Abu.Sikder\\OneDrive - FDA\\data_move_june_2025\\FDA\\EduRAG\\dumps\\s13031-018-0151-3.pdf\"  # ← Change this to your PDF path\n",
    "\n",
    "# Extract text\n",
    "print(f\"Loading PDF: {pdf_path}\")\n",
    "doc = fitz.open(pdf_path)\n",
    "text = \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "\n",
    "print(f\"Extracted {len(text)} characters from {doc.page_count} pages\")\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "Cleaned text: 40240 characters\n",
      "\n",
      "First 300 chars:\n",
      "RESEARCH\n",
      "Open Access\n",
      "Water, sanitation, and hygiene access in southern Syria: analysis of survey data and recommendations for response\n",
      "Mustafa Sikder1*, Umar Daraz2, Daniele Lantagne1 and Roberto Saltori2\n",
      "Abstract\n",
      "Background: Water, sanitation, and hygiene (WASH) are immediate priorities for human s...\n"
     ]
    }
   ],
   "source": [
    "# Clean text (remove PDF artifacts, boilerplate)\n",
    "print(\"Cleaning text...\")\n",
    "text = clean_academic_text(text)\n",
    "\n",
    "print(f\"Cleaned text: {len(text)} characters\")\n",
    "print(f\"\\nFirst 300 chars:\\n{text[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adaptive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chunks...\n",
      "\n",
      "============================================================\n",
      "CHUNKING STATISTICS\n",
      "============================================================\n",
      "Total chunks: 50\n",
      "Average chunk size: 1090 chars\n",
      "Min chunk size: 324 chars\n",
      "Max chunk size: 3999 chars\n",
      "Average sentences per chunk: 4.6\n",
      "\n",
      "Chunks by section:\n",
      "  Abstract: 3 chunks\n",
      "  Introduction: 8 chunks\n",
      "  Methods: 5 chunks\n",
      "  Results: 20 chunks\n",
      "  Discussion: 14 chunks\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize chunker\n",
    "chunker = AcademicPaperChunker(\n",
    "    target_chunk_size=600,\n",
    "    min_chunk_size=200,\n",
    "    max_chunk_size=1000,\n",
    "    overlap_sentences=1,\n",
    "    fallback_to_paragraphs=True\n",
    ")\n",
    "\n",
    "# Create chunks\n",
    "print(\"Creating chunks...\")\n",
    "chunk_objects = chunker.chunk_paper(text)\n",
    "chunks = chunker.chunks_to_list(chunk_objects)\n",
    "\n",
    "# Print statistics\n",
    "chunker.print_statistics(chunk_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1 [Section: abstract]:\n",
      "Background: Water, sanitation, and hygiene (WASH) are immediate priorities for human survival and dignity in emergencies. In 2010, > 90% of Syrians ha...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 [Section: abstract]:\n",
      "Results: In 2016 and 2017, 1281 and 1360 surveys were conducted. Piped water as the main water source declined from\n",
      "22.0% to 15.3% over this time. Hou...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 [Section: abstract]:\n",
      "Conclusions: The private sector has effectively replaced decaying infrastructure in Syria, although at high cost and uncertain quality. Allowing marke...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspect first few chunks\n",
    "print(\"\\nSample chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    section = chunk_objects[i].section\n",
    "    print(f\"Chunk {i+1} [Section: {section}]:\")\n",
    "    print(f\"{chunk[:150]}...\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Index Documents (Hybrid Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
      "Initializing BM25...\n",
      "Indexing documents...\n",
      "\n",
      "Indexing 50 documents...\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cec7d3d82ec49e1916ef0a4d51cfae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing embeddings for cosine similarity...\n",
      "Dense index created with 50 vectors\n",
      "Fitting BM25 on corpus...\n",
      "BM25 fitted successfully\n",
      "\n",
      "✅ Indexing complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize hybrid retriever (dense + sparse)\n",
    "retriever = HybridRetriever(\n",
    "    embedding_model_name='sentence-transformers/multi-qa-mpnet-base-cos-v1',\n",
    "    use_cosine=True,\n",
    "    bm25_k1=1.5,\n",
    "    bm25_b=0.75\n",
    ")\n",
    "\n",
    "# Index documents\n",
    "print(\"Indexing documents...\")\n",
    "retriever.index_documents(chunks, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded (text-generation). Max input tokens: 2048\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM for answer generation\n",
    "generator = RAGAnswerGenerator(\n",
    "    # model_name=\"google/long-t5-tglobal-base\",  # for long context\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # for long context\n",
    "    device=-1  # CPU (use 0 for GPU)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ask Questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, k=3, method='hybrid', show_chunks=True):\n",
    "    \"\"\"\n",
    "    Ask a question about the paper.\n",
    "    \n",
    "    Args:\n",
    "        query: Your question\n",
    "        k: Number of chunks to retrieve\n",
    "        method: 'hybrid', 'dense', or 'sparse'\n",
    "        show_chunks: Whether to display retrieved chunks\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUESTION: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # 1. Retrieve relevant chunks\n",
    "    retrieved_chunks, scores, indices = retriever.search(\n",
    "        query,\n",
    "        k=k,\n",
    "        method=method,\n",
    "        dense_weight=0.6,\n",
    "        sparse_weight=0.4,\n",
    "        fusion_method='weighted'\n",
    "    )\n",
    "    \n",
    "    # 2. Display retrieved chunks (optional)\n",
    "    if show_chunks:\n",
    "        print(\"RETRIEVED CHUNKS:\\n\")\n",
    "        for i, (chunk, score, idx) in enumerate(zip(retrieved_chunks, scores, indices)):\n",
    "            section = chunk_objects[idx].section\n",
    "            preview = chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
    "            print(f\"  {i+1}. [Section: {section}, Score: {score:.4f}, Index: {idx}]\")\n",
    "            print(f\"     {preview}\\n\")\n",
    "    \n",
    "    # 3. Generate answer\n",
    "    print(\"GENERATING ANSWER...\\n\")\n",
    "    answer = generator.generate_answer(\n",
    "        query,\n",
    "        retrieved_chunks,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "        ensure_complete=True, \n",
    "        display_truncation_message=True\n",
    "    )\n",
    "    \n",
    "    # 4. Display answer\n",
    "    # print(f\"ANSWER:\\n\")\n",
    "    # print(f\"{answer}\\n\")\n",
    "    # print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return answer, retrieved_chunks, scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION: What is the main research question or objective of this study?\n",
      "================================================================================\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "  1. [Section: discussion, Score: 0.7609, Index: 44]\n",
      "     The limitations of this analysis include: 1) southern Syria is not representative of all Syria, espe...\n",
      "\n",
      "  2. [Section: results, Score: 0.4000, Index: 21]\n",
      "     Overall, 4.1% of households reported leaving their garbage in the open, and 35.4% were in communitie...\n",
      "\n",
      "  3. [Section: discussion, Score: 0.3963, Index: 43]\n",
      "     Communitylevel WSP programming also reaches a scale that allows cost-effective use of limited resour...\n",
      "\n",
      "GENERATING ANSWER...\n",
      "\n",
      "Input: 851 tokens | Generating up to 200 tokens\n",
      "⚠️  Truncated at '\n",
      "\n",
      "Question:' to prevent over-generation\n",
      "ANSWER:\n",
      "\n",
      "The main research question or objective of this study is to assess the effectiveness of community-level water safety plan (WSP) programming in improving water supply and hygiene practices in southern Syria.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Main research question\n",
    "answers = ask_question(\"What is the main research question or objective of this study?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION: What methods were used to collect data in this study?\n",
      "================================================================================\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "  1. [Section: methods, Score: 1.0000, Index: 14]\n",
      "     Please note the survey tool is available upon request from the corresponding author. Water quality t...\n",
      "\n",
      "  2. [Section: methods, Score: 0.4861, Index: 13]\n",
      "     Enumerators were trained on ethical survey administration; local community councils were informed, a...\n",
      "\n",
      "  3. [Section: methods, Score: 0.2723, Index: 12]\n",
      "     Sample size was calculated using the Krejcie and\n",
      "Morgan model [18]; set for 95% confidence with 10%\n",
      "...\n",
      "\n",
      "GENERATING ANSWER...\n",
      "\n",
      "Input: 690 tokens | Generating up to 200 tokens\n",
      "⚠️  Truncated at '\n",
      "\n",
      "Question:' to prevent over-generation\n",
      "ANSWER:\n",
      "\n",
      "The survey tool is available upon request from the corresponding author.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Methods\n",
    "answers= ask_question(\"What methods were used to collect data in this study?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION: What were the most significant results or findings?\n",
      "================================================================================\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "  1. [Section: results, Score: 0.7553, Index: 31]\n",
      "     IncomeSpent was significant, with OR near one (1.03, 1.02–1.04). These four variables remained signi...\n",
      "\n",
      "  2. [Section: methods, Score: 0.6000, Index: 14]\n",
      "     Please note the survey tool is available upon request from the corresponding author. Water quality t...\n",
      "\n",
      "  3. [Section: results, Score: 0.3475, Index: 30]\n",
      "     In 2016, 10 of the 15 variables were significantly associated with diarrhea in children < 5 in univa...\n",
      "\n",
      "GENERATING ANSWER...\n",
      "\n",
      "Input: 1143 tokens | Generating up to 200 tokens\n",
      "⚠️  Truncated at '\n",
      "\n",
      "Question:' to prevent over-generation\n",
      "ANSWER:\n",
      "\n",
      "The most significant results or findings were that the protective factor was FunctionalToilet (mixed effect OR (mOR): 0.62 (95% CI 0.46–0.82)), and risk factors included AdequateWater (2.14, 1.62–2.84) and SeparateWater (2.03, 1.52–2.72).\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Results\n",
    "answers= ask_question(\"What were the most significant results or findings?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION: What limitations or weaknesses does the study acknowledge?\n",
      "================================================================================\n",
      "\n",
      "RETRIEVED CHUNKS:\n",
      "\n",
      "  1. [Section: discussion, Score: 0.8842, Index: 44]\n",
      "     The limitations of this analysis include: 1) southern Syria is not representative of all Syria, espe...\n",
      "\n",
      "  2. [Section: discussion, Score: 0.6589, Index: 43]\n",
      "     Communitylevel WSP programming also reaches a scale that allows cost-effective use of limited resour...\n",
      "\n",
      "  3. [Section: discussion, Score: 0.4226, Index: 48]\n",
      "     The lesson from the Syria WASH response is that allowing market forces to manage services and quanti...\n",
      "\n",
      "GENERATING ANSWER...\n",
      "\n",
      "Input: 696 tokens | Generating up to 200 tokens\n",
      "ℹ️  Truncated to last complete sentence\n",
      "ANSWER:\n",
      "\n",
      "The study acknowledges the following limitations: 1) Southern Syria is not representative of all Syria, especially for power supply, and therefore water network, availability. 2) No microbiological water quality data was collected, although FCR presence is an indicator of no/low bacterial contamination. 3) Self-reported diarrhea is an indicator subject to response bias, and no standard definition of diarrhea was provided to respondents. 4) Sample size calculation was not specifically completed for households with children < 5 and we removed a large number of households from the dataset. 5) Data are from cross-sectional surveys, not experimental evaluations; thus, causation cannot be determined.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Limitations\n",
    "answers= ask_question(\"What limitations or weaknesses does the study acknowledge?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question!\n",
    "my_question = \"Your question here\"  # ← Change this\n",
    "answers= ask_question(my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Retrieval Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARING RETRIEVAL METHODS\n",
      "Query: What methods were used?\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUERY: What methods were used?\n",
      "================================================================================\n",
      "\n",
      "DENSE RETRIEVAL:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Rank 1 | Score: 0.3212 | Chunk ID: 14\n",
      "Text: Please note the survey tool is available upon request from the corresponding author. Water quality testing\n",
      "During the survey, drinking water samples f...\n",
      "\n",
      "Rank 2 | Score: 0.3057 | Chunk ID: 13\n",
      "Text: Enumerators were trained on ethical survey administration; local community councils were informed, and household consent was obtained before conductin...\n",
      "\n",
      "Rank 3 | Score: 0.2849 | Chunk ID: 10\n",
      "Text: Depending on the situation, WSPs vary in complexity. In case of southern Syria, WSP implementation involved conducing a risk assessment at three level...\n",
      "\n",
      "\n",
      "SPARSE RETRIEVAL:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Rank 1 | Score: 3.9904 | Chunk ID: 40\n",
      "Text: 3) How can community-level WASH interventions, such as WSPs, be scaled up? and, 4) What are the appropriate criteria to use to prioritize beneficiarie...\n",
      "\n",
      "Rank 2 | Score: 3.8381 | Chunk ID: 21\n",
      "Text: Overall, 4.1% of households reported leaving their garbage in the open, and 35.4% were in communities targeted with Water Safety Plan programming. Whe...\n",
      "\n",
      "Rank 3 | Score: 3.2897 | Chunk ID: 22\n",
      "Text: The respondents who answered they could not find or afford items were asked in follow-up what items they\n",
      "Al−Khashniyyeh\n",
      "As−Sanamayn\n",
      "Busra Esh−Sham\n",
      "Da'...\n",
      "\n",
      "\n",
      "HYBRID RETRIEVAL:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Rank 1 | Score: 0.0313 | Chunk ID: 14\n",
      "Text: Please note the survey tool is available upon request from the corresponding author. Water quality testing\n",
      "During the survey, drinking water samples f...\n",
      "\n",
      "Rank 2 | Score: 0.0312 | Chunk ID: 12\n",
      "Text: Sample size was calculated using the Krejcie and\n",
      "Morgan model [18]; set for 95% confidence with 10%\n",
      "margin of error and one degree of freedom, to allo...\n",
      "\n",
      "Rank 3 | Score: 0.0305 | Chunk ID: 0\n",
      "Text: Background: Water, sanitation, and hygiene (WASH) are immediate priorities for human survival and dignity in emergencies. In 2010, > 90% of Syrians ha...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare dense, sparse, and hybrid retrieval\n",
    "test_query = \"What methods were used?\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPARING RETRIEVAL METHODS\")\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "retriever.compare_methods(test_query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. Extracted and cleaned text from a PDF\n",
    "2. Created adaptive semantic chunks\n",
    "3. Built a hybrid retrieval index (dense + sparse)\n",
    "4. Generated answers using LongT5\n",
    "5. Asked questions about the paper\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different papers\n",
    "- Experiment with retrieval weights (`dense_weight`, `sparse_weight`)\n",
    "- Test different LLM models (`google/flan-t5-xl`, `microsoft/phi-3-mini-4k-instruct`)\n",
    "- Evaluate on the 20 generic questions\n",
    "\n",
    "**For production use:**\n",
    "- See `demo/gradio_app.py` for web interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
